{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBSQNXlCPwcq"
      },
      "source": [
        "## MoCo on CIFAR10 using CrossBatchMemory from [PyTorch Metric Learning](https://github.com/KevinMusgrave/pytorch-metric-learning)\n",
        "Why use CrossBatchMemory?\n",
        "- It separates the queue from the model (it's modular).\n",
        "- You can easily use any tuple loss on the embeddings (not just InfoNCE / NTXent).\n",
        "- You can optionally use any pair/triplet miner to extract hard samples from the momentum encoder queue.\n",
        "\n",
        "After 200 epochs:\n",
        "\n",
        "- This notebook's accuracy: 82.9%\n",
        "- The [official MoCo repo's CIFAR10 notebook](https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb) accuracy: 82.6%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2rhDLFuNtsP"
      },
      "outputs": [],
      "source": [
        "!pip install -q pytorch-metric-learning[with-hooks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uWf-I3fRbZH"
      },
      "source": [
        "# Creating the dataset and image transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjqjvG7eXpxs"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from PIL import ImageFilter\n",
        "\n",
        "##############################################################################\n",
        "### Many parts of this are a modified version of the official MoCo code ######\n",
        "############### https://github.com/facebookresearch/moco #####################\n",
        "##############################################################################\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet\n",
        "from tqdm import tqdm\n",
        "\n",
        "from pytorch_metric_learning import losses\n",
        "from pytorch_metric_learning.utils import logging_presets\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "\n",
        "######################\n",
        "### from MoCo repo ###\n",
        "######################\n",
        "class TwoCropsTransform:\n",
        "    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n",
        "\n",
        "    def __init__(self, base_transform):\n",
        "        self.base_transform = base_transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        q = self.base_transform(x)\n",
        "        k = self.base_transform(x)\n",
        "        return [q, k]\n",
        "\n",
        "\n",
        "######################\n",
        "### from MoCo repo ###\n",
        "######################\n",
        "class GaussianBlur(object):\n",
        "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
        "\n",
        "    def __init__(self, sigma=[0.1, 2.0]):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, x):\n",
        "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
        "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
        "        return x\n",
        "\n",
        "\n",
        "######################\n",
        "### from MoCo repo ###\n",
        "######################\n",
        "def create_dataset(batch_size):\n",
        "    normalize = transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
        "\n",
        "    train_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomResizedCrop(32),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "            transforms.RandomGrayscale(p=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    train_transform = TwoCropsTransform(train_transform)\n",
        "\n",
        "    val_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        \"dataset\", train=True, download=True, transform=train_transform\n",
        "    )\n",
        "    train_dataset_for_eval = datasets.CIFAR10(\n",
        "        \"dataset\", train=True, download=True, transform=val_transform\n",
        "    )\n",
        "    val_dataset = datasets.CIFAR10(\n",
        "        \"dataset\", train=False, download=True, transform=val_transform\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    train_loader_for_eval = torch.utils.data.DataLoader(\n",
        "        train_dataset_for_eval,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        train_dataset,\n",
        "        train_dataset_for_eval,\n",
        "        val_dataset,\n",
        "        train_loader,\n",
        "        train_loader_for_eval,\n",
        "        val_loader,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaZECqCczvkY"
      },
      "source": [
        "# Model definition and code for copying params from encQ to encK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xovt6_yxzsAI"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "### from MoCo repo ###\n",
        "######################\n",
        "# SplitBatchNorm: simulate multi-gpu behavior of BatchNorm in one gpu by splitting alone the batch dimension\n",
        "# implementation adapted from https://github.com/davidcpage/cifar10-fast/blob/master/torch_backend.py\n",
        "class SplitBatchNorm(torch.nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, num_splits, **kw):\n",
        "        super().__init__(num_features, **kw)\n",
        "        self.num_splits = num_splits\n",
        "\n",
        "    def forward(self, input):\n",
        "        N, C, H, W = input.shape\n",
        "        if self.training or not self.track_running_stats:\n",
        "            running_mean_split = self.running_mean.repeat(self.num_splits)\n",
        "            running_var_split = self.running_var.repeat(self.num_splits)\n",
        "            outcome = torch.nn.functional.batch_norm(\n",
        "                input.view(-1, C * self.num_splits, H, W),\n",
        "                running_mean_split,\n",
        "                running_var_split,\n",
        "                self.weight.repeat(self.num_splits),\n",
        "                self.bias.repeat(self.num_splits),\n",
        "                True,\n",
        "                self.momentum,\n",
        "                self.eps,\n",
        "            ).view(N, C, H, W)\n",
        "            self.running_mean.data.copy_(\n",
        "                running_mean_split.view(self.num_splits, C).mean(dim=0)\n",
        "            )\n",
        "            self.running_var.data.copy_(\n",
        "                running_var_split.view(self.num_splits, C).mean(dim=0)\n",
        "            )\n",
        "            return outcome\n",
        "        else:\n",
        "            return torch.nn.functional.batch_norm(\n",
        "                input,\n",
        "                self.running_mean,\n",
        "                self.running_var,\n",
        "                self.weight,\n",
        "                self.bias,\n",
        "                False,\n",
        "                self.momentum,\n",
        "                self.eps,\n",
        "            )\n",
        "\n",
        "\n",
        "######################\n",
        "### from MoCo repo ###\n",
        "######################\n",
        "class ModelBase(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Common CIFAR ResNet recipe.\n",
        "    Comparing with ImageNet ResNet recipe, it:\n",
        "    (i) replaces conv1 with kernel=3, str=1\n",
        "    (ii) removes pool1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim=128, arch=\"resnet18\", bn_splits=8):\n",
        "        super(ModelBase, self).__init__()\n",
        "\n",
        "        # use split batchnorm\n",
        "        norm_layer = (\n",
        "            partial(SplitBatchNorm, num_splits=bn_splits)\n",
        "            if bn_splits > 1\n",
        "            else torch.nn.BatchNorm2d\n",
        "        )\n",
        "        resnet_arch = getattr(resnet, arch)\n",
        "        net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n",
        "\n",
        "        self.net = []\n",
        "        for name, module in net.named_children():\n",
        "            if name == \"conv1\":\n",
        "                module = torch.nn.Conv2d(\n",
        "                    3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
        "                )\n",
        "            if isinstance(module, torch.nn.MaxPool2d):\n",
        "                continue\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                self.net.append(torch.nn.Flatten(1))\n",
        "            self.net.append(module)\n",
        "\n",
        "        self.net = torch.nn.Sequential(*self.net)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        # note: not normalized here\n",
        "        return x\n",
        "\n",
        "\n",
        "######################\n",
        "### from MoCo repo ###\n",
        "######################\n",
        "def copy_params(encQ, encK, m=None):\n",
        "    if m is None:\n",
        "        for param_q, param_k in zip(encQ.parameters(), encK.parameters()):\n",
        "            param_k.data.copy_(param_q.data)  # initialize\n",
        "            param_k.requires_grad = False  # not update by gradient\n",
        "    else:\n",
        "        for param_q, param_k in zip(encQ.parameters(), encK.parameters()):\n",
        "            param_k.data = param_k.data * m + param_q.data * (1.0 - m)\n",
        "\n",
        "\n",
        "def create_encoder():\n",
        "    model = ModelBase()\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    model.to(device)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhSIo_px0A9Y"
      },
      "source": [
        "# Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqm_tEANz-OJ"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "### from MoCo repo ###\n",
        "######################\n",
        "# test using a knn monitor\n",
        "def test(net, memory_data_loader, test_data_loader, epoch, knn_k, knn_t, record_keeper):\n",
        "    net.eval()\n",
        "    classes = len(memory_data_loader.dataset.classes)\n",
        "    total_top1, total_num, feature_bank = 0.0, 0, []\n",
        "    with torch.no_grad():\n",
        "        # generate feature bank\n",
        "        for data, target in tqdm(memory_data_loader, desc=\"Feature extracting\"):\n",
        "            feature = net(data.cuda(non_blocking=True))\n",
        "            feature = torch.nn.functional.normalize(feature, dim=1)\n",
        "            feature_bank.append(feature)\n",
        "        # [D, N]\n",
        "        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n",
        "        # [N]\n",
        "        feature_labels = torch.tensor(\n",
        "            memory_data_loader.dataset.targets, device=feature_bank.device\n",
        "        )\n",
        "        # loop test data to predict the label by weighted knn search\n",
        "        test_bar = tqdm(test_data_loader)\n",
        "        for data, target in test_bar:\n",
        "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "            feature = net(data)\n",
        "            feature = torch.nn.functional.normalize(feature, dim=1)\n",
        "\n",
        "            pred_labels = knn_predict(\n",
        "                feature, feature_bank, feature_labels, classes, knn_k, knn_t\n",
        "            )\n",
        "\n",
        "            total_num += data.size(0)\n",
        "            total_top1 += (pred_labels[:, 0] == target).float().sum().item()\n",
        "            acc = total_top1 / total_num * 100\n",
        "            test_bar.set_description(\"Test Epoch {}: Acc@1:{:.2f}%\".format(epoch, acc))\n",
        "\n",
        "    record_keeper.update_records(\n",
        "        {\"knn_monitor_accuracy\": acc},\n",
        "        epoch,\n",
        "        parent_name=\"accuracy\",\n",
        "    )\n",
        "    record_keeper.save_records()\n",
        "    return acc\n",
        "\n",
        "\n",
        "######################\n",
        "### from MoCo repo ###\n",
        "######################\n",
        "# knn monitor as in InstDisc https://arxiv.org/abs/1805.01978\n",
        "# implementation follows http://github.com/zhirongw/lemniscate.pytorch and https://github.com/leftthomas/SimCLR\n",
        "def knn_predict(feature, feature_bank, feature_labels, classes, knn_k, knn_t):\n",
        "    # compute cos similarity between each feature vector and feature bank ---> [B, N]\n",
        "    sim_matrix = torch.mm(feature, feature_bank)\n",
        "    # [B, K]\n",
        "    sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)\n",
        "    # [B, K]\n",
        "    sim_labels = torch.gather(\n",
        "        feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices\n",
        "    )\n",
        "    sim_weight = (sim_weight / knn_t).exp()\n",
        "\n",
        "    # counts for each class\n",
        "    one_hot_label = torch.zeros(\n",
        "        feature.size(0) * knn_k, classes, device=sim_labels.device\n",
        "    )\n",
        "    # [B*K, C]\n",
        "    one_hot_label = one_hot_label.scatter(\n",
        "        dim=-1, index=sim_labels.view(-1, 1), value=1.0\n",
        "    )\n",
        "    # weighted score ---> [B, C]\n",
        "    pred_scores = torch.sum(\n",
        "        one_hot_label.view(feature.size(0), -1, classes) * sim_weight.unsqueeze(dim=-1),\n",
        "        dim=1,\n",
        "    )\n",
        "\n",
        "    pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
        "    return pred_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-g8Vu7o0F8V"
      },
      "source": [
        "# Training and logging functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4nsv5GA0GDh"
      },
      "outputs": [],
      "source": [
        "def update_records(loss, loss_fn, optimizer, record_keeper, global_iteration):\n",
        "    def optimizer_custom_attr_func(opt):\n",
        "        return {\"lr\": opt.param_groups[0][\"lr\"]}\n",
        "\n",
        "    record_these = [\n",
        "        [{\"loss\": loss.item()}, {\"parent_name\": \"loss_histories\"}],\n",
        "        [{\"loss_function\": loss_fn}, {\"recursive_types\": [torch.nn.Module]}],\n",
        "        [{\"optimizer\": optimizer}, {\"custom_attr_func\": optimizer_custom_attr_func}],\n",
        "    ]\n",
        "    for record, kwargs in record_these:\n",
        "        record_keeper.update_records(record, global_iteration, **kwargs)\n",
        "\n",
        "\n",
        "def save_model(encQ):\n",
        "    model_folder = \"example_saved_models\"\n",
        "    if not os.path.exists(model_folder):\n",
        "        os.makedirs(model_folder)\n",
        "    torch.save(encQ.state_dict(), \"{}/encQ_best.pth\".format(model_folder))\n",
        "\n",
        "\n",
        "######################\n",
        "### from MoCo repo ###\n",
        "######################\n",
        "def batch_shuffle_single_gpu(x):\n",
        "    \"\"\"\n",
        "    Batch shuffle, for making use of BatchNorm.\n",
        "    \"\"\"\n",
        "    # random shuffle index\n",
        "    idx_shuffle = torch.randperm(x.shape[0]).cuda()\n",
        "\n",
        "    # index for restoring\n",
        "    idx_unshuffle = torch.argsort(idx_shuffle)\n",
        "\n",
        "    return x[idx_shuffle], idx_unshuffle\n",
        "\n",
        "\n",
        "######################\n",
        "### from MoCo repo ###\n",
        "######################\n",
        "def batch_unshuffle_single_gpu(x, idx_unshuffle):\n",
        "    \"\"\"\n",
        "    Undo batch shuffle.\n",
        "    \"\"\"\n",
        "    return x[idx_unshuffle]\n",
        "\n",
        "\n",
        "def create_labels(num_pos_pairs, previous_max_label):\n",
        "    # create labels that indicate what the positive pairs are\n",
        "    labels = torch.arange(0, num_pos_pairs)\n",
        "    labels = torch.cat((labels, labels)).to(device)\n",
        "    # add an offset so that the labels do not overlap with any labels in the memory queue\n",
        "    labels += previous_max_label + 1\n",
        "    # we want to enqueue the output of encK, which is the 2nd half of the batch\n",
        "    enqueue_mask = torch.zeros(len(labels)).bool()\n",
        "    enqueue_mask[num_pos_pairs:] = True\n",
        "    return labels, enqueue_mask\n",
        "\n",
        "\n",
        "def train(\n",
        "    encQ,\n",
        "    encK,\n",
        "    paramK_momentum,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    train_loader,\n",
        "    record_keeper,\n",
        "    global_iteration,\n",
        "):\n",
        "    encQ.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for images, _ in pbar:\n",
        "        previous_max_label = torch.max(loss_fn.label_memory)\n",
        "        imgQ = images[0].to(device)\n",
        "        imgK = images[1].to(device)\n",
        "\n",
        "        # compute output\n",
        "        encQ_out = encQ(imgQ)\n",
        "        with torch.no_grad():  # no gradient to keys\n",
        "            copy_params(encQ, encK, m=paramK_momentum)\n",
        "            imgK, idx_unshuffle = batch_shuffle_single_gpu(imgK)\n",
        "            encK_out = encK(imgK)\n",
        "            encK_out = batch_unshuffle_single_gpu(encK_out, idx_unshuffle)\n",
        "\n",
        "        all_enc = torch.cat([encQ_out, encK_out], dim=0)\n",
        "        labels, enqueue_mask = create_labels(encQ_out.size(0), previous_max_label)\n",
        "        loss = loss_fn(all_enc, labels, enqueue_mask=enqueue_mask)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_description(\"loss=%.5f\" % loss.item())\n",
        "        update_records(\n",
        "            loss, loss_fn, optimizer, record_keeper, global_iteration[\"iter\"]\n",
        "        )\n",
        "        global_iteration[\"iter\"] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U21IlMofRoJ7"
      },
      "source": [
        "# Start Tensorboard and delete old folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u9YpcX7Oo7o"
      },
      "outputs": [],
      "source": [
        "!rm -rfv example_logs example_tensorboard example_saved_models\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir example_tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pc3ReL0Rvi3"
      },
      "source": [
        "# Set parameters and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A1sKW7N8_tk"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "lr = 0.03\n",
        "paramK_momentum = 0.99\n",
        "memory_size = 4096\n",
        "num_epochs = 200\n",
        "knn_k = 200\n",
        "knn_t = 0.1\n",
        "\n",
        "(\n",
        "    train_dataset,\n",
        "    train_dataset_for_eval,\n",
        "    val_dataset,\n",
        "    train_loader,\n",
        "    train_loader_for_eval,\n",
        "    val_loader,\n",
        ") = create_dataset(batch_size)\n",
        "\n",
        "encQ = create_encoder()\n",
        "encK = create_encoder()\n",
        "\n",
        "# copy params from encQ into encK\n",
        "copy_params(encQ, encK)\n",
        "\n",
        "optimizer = torch.optim.SGD(encQ.parameters(), lr, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
        "\n",
        "###########################################################\n",
        "### Set the loss function and the (optional) miner here ###\n",
        "###########################################################\n",
        "loss_fn = losses.CrossBatchMemory(\n",
        "    loss=losses.NTXentLoss(temperature=0.1), embedding_size=128, memory_size=memory_size\n",
        ")\n",
        "\n",
        "dataset_dict = {\"train\": train_dataset_for_eval, \"val\": val_dataset}\n",
        "record_keeper, _, _ = logging_presets.get_record_keeper(\n",
        "    \"example_logs\", \"example_tensorboard\"\n",
        ")\n",
        "hooks = logging_presets.get_hook_container(record_keeper)\n",
        "\n",
        "# first check untrained performance\n",
        "epoch = 0\n",
        "best_accuracy = test(\n",
        "    encQ, train_loader_for_eval, val_loader, epoch, knn_k, knn_t, record_keeper\n",
        ")\n",
        "\n",
        "global_iteration = {\"iter\": 0}\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    logging.info(\"Starting epoch {}\".format(epoch))\n",
        "    train(\n",
        "        encQ,\n",
        "        encK,\n",
        "        paramK_momentum,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        train_loader,\n",
        "        record_keeper,\n",
        "        global_iteration,\n",
        "    )\n",
        "    curr_accuracy = test(\n",
        "        encQ, train_loader_for_eval, val_loader, epoch, knn_k, knn_t, record_keeper\n",
        "    )\n",
        "    if curr_accuracy > best_accuracy:\n",
        "        best_accuracy = curr_accuracy\n",
        "        save_model(encQ)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFufpRCEM_qe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入问题所需要的关键包\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "from utils import plot_image, plot_curve, one_hot\n",
        "\n",
        "# step 1.load dataset加载数据集\n",
        "train_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('mnist_data', train=True, download=False, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,),(0.3081,))])), batch_size=1, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('mnist_data/', train=False, download=False, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,),(0.3081,))])), batch_size=1, shuffle=False)\n",
        "\n",
        "x, y = next(iter(train_loader))\n",
        "print(x.shape, y.shape, x.min(), x.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1QLEkSNOMRX",
        "outputId": "b5780b44-6ed7-4e3c-c051-db735a2712c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 28, 28]) torch.Size([1]) tensor(-0.4242) tensor(2.8215)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_image(x, y, 'image sample')\n",
        ""
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "27UTtPvMwn2z",
        "outputId": "14ba4596-0a29-4f45-aa57-79bf516e8ea9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 1 is out of bounds for dimension 0 with size 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-91944c3cd0aa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image sample'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mplot_image\u001b[0;34m(img, label, name)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# plt.subplot(2, 3, i + 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.3081\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.1307\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 构建网络模型\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net,self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# 进行网络的初始化\n",
        "net = Net().to(\"cuda\")\n",
        "# 定义优化器\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
        "# 初始化数组存储loss数值便于绘图\n",
        "train_loss =[]\n",
        "\n",
        "# 对数据集迭代\n",
        "for epoch in range(1):\n",
        "    # 对batch迭代\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "      if batch_idx > 1e4: break\n",
        "      # x:[b,1,28,28],y:[512]\n",
        "      # [b, 784]\n",
        "      x = x.view(x.size(0), 28*28).to(\"cuda\")\n",
        "      # [b, 10]\n",
        "      out = net(x)\n",
        "      y_onehot = one_hot(y).to(\"cuda\")\n",
        "      # loss=mse(out, y_onehot) 计算loss数值\n",
        "      loss = F.mse_loss(out, y_onehot)\n",
        "      # 将梯度置零操作\n",
        "      optimizer.zero_grad()\n",
        "      # 反向传播\n",
        "      loss.backward()\n",
        "      # 更新权重值\n",
        "      optimizer.step()\n",
        "      # 累加loss值\n",
        "      train_loss.append(loss.item())\n",
        "      # 每十个batch进行loss值打印\n",
        "      if batch_idx % 10 ==0:\n",
        "          print(epoch, batch_idx, loss.item())\n",
        "\n",
        "# # 绘制loss曲线\n",
        "# plot_curve(train_loss)\n",
        "\n",
        "# # 进行测试\n",
        "# total_correct = 0\n",
        "# for x, y in test_loader:\n",
        "#     x = x.view(x.size(0), 28*28)\n",
        "#     out = net(x)\n",
        "#     # out[b, 10], pred[b]\n",
        "#     pred = out.argmax(dim=1)\n",
        "#     correct = pred.eq(y).sum().float()\n",
        "#     totol_correct += correct\n",
        "\n",
        "# total_num = len(test_loader.dataset)\n",
        "# acc = total_correct / total_num\n",
        "# print('test acc:', acc)\n",
        "\n",
        "# # 进行样例打印\n",
        "# x, y = next(iter(test_loader))\n",
        "# out = net(x.view(x.size(0), 28*28))\n",
        "# pred = out.argmax(dim=1)\n",
        "# plot_image(x, pred, 'test')"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2vMSIcLwjvN",
        "outputId": "078c4276-dd42-4489-a0f9-d8e78646d57b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 0.1168004646897316\n",
            "0 10 0.04757364094257355\n",
            "0 20 0.13495834171772003\n",
            "0 30 0.1425168514251709\n",
            "0 40 0.10365035384893417\n",
            "0 50 0.09847479313611984\n",
            "0 60 0.096549853682518\n",
            "0 70 0.07275830954313278\n",
            "0 80 0.10660126060247421\n",
            "0 90 0.12021558731794357\n",
            "0 100 0.07760921865701675\n",
            "0 110 0.0965314507484436\n",
            "0 120 0.11163824051618576\n",
            "0 130 0.10442578047513962\n",
            "0 140 0.07549736648797989\n",
            "0 150 0.11839145421981812\n",
            "0 160 0.0702194795012474\n",
            "0 170 0.10788489878177643\n",
            "0 180 0.08169298619031906\n",
            "0 190 0.0954233705997467\n",
            "0 200 0.10350115597248077\n",
            "0 210 0.12605732679367065\n",
            "0 220 0.09234599024057388\n",
            "0 230 0.09036607295274734\n",
            "0 240 0.10201163589954376\n",
            "0 250 0.13261544704437256\n",
            "0 260 0.11026287078857422\n",
            "0 270 0.11211691051721573\n",
            "0 280 0.13782738149166107\n",
            "0 290 0.102610282599926\n",
            "0 300 0.08393917977809906\n",
            "0 310 0.10111068934202194\n",
            "0 320 0.11114108562469482\n",
            "0 330 0.08721322566270828\n",
            "0 340 0.09845591336488724\n",
            "0 350 0.10278737545013428\n",
            "0 360 0.10239312797784805\n",
            "0 370 0.06274015456438065\n",
            "0 380 0.06713701784610748\n",
            "0 390 0.12895715236663818\n",
            "0 400 0.09992003440856934\n",
            "0 410 0.08522602170705795\n",
            "0 420 0.13202477991580963\n",
            "0 430 0.11544598639011383\n",
            "0 440 0.06906414777040482\n",
            "0 450 0.08685388416051865\n",
            "0 460 0.10145977884531021\n",
            "0 470 0.039957195520401\n",
            "0 480 0.13479852676391602\n",
            "0 490 0.09852614998817444\n",
            "0 500 0.11391563713550568\n",
            "0 510 0.10930266231298447\n",
            "0 520 0.07339688390493393\n",
            "0 530 0.07441405206918716\n",
            "0 540 0.10002702474594116\n",
            "0 550 0.11880256235599518\n",
            "0 560 0.08778461068868637\n",
            "0 570 0.06910623610019684\n",
            "0 580 0.07849154621362686\n",
            "0 590 0.07416202127933502\n",
            "0 600 0.11703365296125412\n",
            "0 610 0.09057644009590149\n",
            "0 620 0.0907520055770874\n",
            "0 630 0.10298997163772583\n",
            "0 640 0.11438234150409698\n",
            "0 650 0.10772400349378586\n",
            "0 660 0.09295850992202759\n",
            "0 670 0.09318451583385468\n",
            "0 680 0.11467526108026505\n",
            "0 690 0.08475909382104874\n",
            "0 700 0.11587178707122803\n",
            "0 710 0.09142918139696121\n",
            "0 720 0.07209035754203796\n",
            "0 730 0.13034383952617645\n",
            "0 740 0.12026860564947128\n",
            "0 750 0.09720972925424576\n",
            "0 760 0.11448042839765549\n",
            "0 770 0.06815927475690842\n",
            "0 780 0.12000489234924316\n",
            "0 790 0.08144286274909973\n",
            "0 800 0.09902462363243103\n",
            "0 810 0.11925911158323288\n",
            "0 820 0.09053508192300797\n",
            "0 830 0.0794944167137146\n",
            "0 840 0.08508534729480743\n",
            "0 850 0.09750481694936752\n",
            "0 860 0.0940009132027626\n",
            "0 870 0.09549867361783981\n",
            "0 880 0.0740831270813942\n",
            "0 890 0.10619697719812393\n",
            "0 900 0.12996752560138702\n",
            "0 910 0.10276257991790771\n",
            "0 920 0.11926720291376114\n",
            "0 930 0.11444929987192154\n",
            "0 940 0.08452725410461426\n",
            "0 950 0.113273024559021\n",
            "0 960 0.09241236746311188\n",
            "0 970 0.12254804372787476\n",
            "0 980 0.11146853119134903\n",
            "0 990 0.06371194869279861\n",
            "0 1000 0.06453662365674973\n",
            "0 1010 0.08839988708496094\n",
            "0 1020 0.08602295070886612\n",
            "0 1030 0.08578707277774811\n",
            "0 1040 0.10521083325147629\n",
            "0 1050 0.08539599180221558\n",
            "0 1060 0.1033535748720169\n",
            "0 1070 0.08655449002981186\n",
            "0 1080 0.09518361836671829\n",
            "0 1090 0.12096401304006577\n",
            "0 1100 0.11194107681512833\n",
            "0 1110 0.13732224702835083\n",
            "0 1120 0.09929851442575455\n",
            "0 1130 0.07740367949008942\n",
            "0 1140 0.11222219467163086\n",
            "0 1150 0.08829211443662643\n",
            "0 1160 0.09733801335096359\n",
            "0 1170 0.06879374384880066\n",
            "0 1180 0.08093352615833282\n",
            "0 1190 0.10644636303186417\n",
            "0 1200 0.08103328198194504\n",
            "0 1210 0.11425717175006866\n",
            "0 1220 0.09799934178590775\n",
            "0 1230 0.10069223493337631\n",
            "0 1240 0.07568574696779251\n",
            "0 1250 0.08911668509244919\n",
            "0 1260 0.11344722658395767\n",
            "0 1270 0.09126681089401245\n",
            "0 1280 0.09843426197767258\n",
            "0 1290 0.08919988572597504\n",
            "0 1300 0.13664989173412323\n",
            "0 1310 0.1154271587729454\n",
            "0 1320 0.11946771293878555\n",
            "0 1330 0.1067928671836853\n",
            "0 1340 0.08674616366624832\n",
            "0 1350 0.1020476445555687\n",
            "0 1360 0.08593872934579849\n",
            "0 1370 0.05132642015814781\n",
            "0 1380 0.11124755442142487\n",
            "0 1390 0.11583828181028366\n",
            "0 1400 0.06887486577033997\n",
            "0 1410 0.12089518457651138\n",
            "0 1420 0.08537092059850693\n",
            "0 1430 0.09820380061864853\n",
            "0 1440 0.056610364466905594\n",
            "0 1450 0.09666962921619415\n",
            "0 1460 0.0882069393992424\n",
            "0 1470 0.11050692945718765\n",
            "0 1480 0.11739635467529297\n",
            "0 1490 0.08699574321508408\n",
            "0 1500 0.06730682402849197\n",
            "0 1510 0.1040361300110817\n",
            "0 1520 0.09078767895698547\n",
            "0 1530 0.11568110436201096\n",
            "0 1540 0.12335624545812607\n",
            "0 1550 0.09210788458585739\n",
            "0 1560 0.09573561698198318\n",
            "0 1570 0.10461658239364624\n",
            "0 1580 0.12552712857723236\n",
            "0 1590 0.1045980229973793\n",
            "0 1600 0.09918390214443207\n",
            "0 1610 0.07574095577001572\n",
            "0 1620 0.12075471132993698\n",
            "0 1630 0.08889928460121155\n",
            "0 1640 0.10705669224262238\n",
            "0 1650 0.08256518095731735\n",
            "0 1660 0.10212788730859756\n",
            "0 1670 0.101520836353302\n",
            "0 1680 0.1145055815577507\n",
            "0 1690 0.08032180368900299\n",
            "0 1700 0.08545048534870148\n",
            "0 1710 0.10800065100193024\n",
            "0 1720 0.0953008159995079\n",
            "0 1730 0.07707726955413818\n",
            "0 1740 0.08045095950365067\n",
            "0 1750 0.12077143043279648\n",
            "0 1760 0.10430058091878891\n",
            "0 1770 0.10366892069578171\n",
            "0 1780 0.13824176788330078\n",
            "0 1790 0.08813493698835373\n",
            "0 1800 0.11265305429697037\n",
            "0 1810 0.07480131834745407\n",
            "0 1820 0.08100786805152893\n",
            "0 1830 0.12797372043132782\n",
            "0 1840 0.09498216956853867\n",
            "0 1850 0.06916492432355881\n",
            "0 1860 0.1193879172205925\n",
            "0 1870 0.06399188935756683\n",
            "0 1880 0.08997862786054611\n",
            "0 1890 0.09078187495470047\n",
            "0 1900 0.09594708681106567\n",
            "0 1910 0.09195761382579803\n",
            "0 1920 0.10792406648397446\n",
            "0 1930 0.09234964102506638\n",
            "0 1940 0.10368894785642624\n",
            "0 1950 0.07884842157363892\n",
            "0 1960 0.12175240367650986\n",
            "0 1970 0.12485971301794052\n",
            "0 1980 0.09950427711009979\n",
            "0 1990 0.11058806627988815\n",
            "0 2000 0.125997856259346\n",
            "0 2010 0.10881859064102173\n",
            "0 2020 0.1061398983001709\n",
            "0 2030 0.03848239779472351\n",
            "0 2040 0.09395240247249603\n",
            "0 2050 0.07595895975828171\n",
            "0 2060 0.06466755270957947\n",
            "0 2070 0.11282030493021011\n",
            "0 2080 0.11373783648014069\n",
            "0 2090 0.08826769888401031\n",
            "0 2100 0.11762410402297974\n",
            "0 2110 0.116881363093853\n",
            "0 2120 0.1336580365896225\n",
            "0 2130 0.05853798985481262\n",
            "0 2140 0.07061626762151718\n",
            "0 2150 0.11285247653722763\n",
            "0 2160 0.08852769434452057\n",
            "0 2170 0.06235278770327568\n",
            "0 2180 0.0990583747625351\n",
            "0 2190 0.10215961188077927\n",
            "0 2200 0.10058140009641647\n",
            "0 2210 0.08836936205625534\n",
            "0 2220 0.09229879081249237\n",
            "0 2230 0.06182297691702843\n",
            "0 2240 0.08015496283769608\n",
            "0 2250 0.11122947186231613\n",
            "0 2260 0.08893110603094101\n",
            "0 2270 0.09728021174669266\n",
            "0 2280 0.11751890182495117\n",
            "0 2290 0.10694058239459991\n",
            "0 2300 0.07730693370103836\n",
            "0 2310 0.10023442655801773\n",
            "0 2320 0.10335617512464523\n",
            "0 2330 0.12795093655586243\n",
            "0 2340 0.10364917665719986\n",
            "0 2350 0.10669505596160889\n",
            "0 2360 0.09662165492773056\n",
            "0 2370 0.06476501375436783\n",
            "0 2380 0.10257849842309952\n",
            "0 2390 0.12589330971240997\n",
            "0 2400 0.08843838423490524\n",
            "0 2410 0.11539068073034286\n",
            "0 2420 0.12274982035160065\n",
            "0 2430 0.11275913566350937\n",
            "0 2440 0.08387134224176407\n",
            "0 2450 0.12876205146312714\n",
            "0 2460 0.08461382985115051\n",
            "0 2470 0.11470947414636612\n",
            "0 2480 0.09156674146652222\n",
            "0 2490 0.08854200690984726\n",
            "0 2500 0.10539393872022629\n",
            "0 2510 0.13140150904655457\n",
            "0 2520 0.08757684379816055\n",
            "0 2530 0.1227630153298378\n",
            "0 2540 0.06597459316253662\n",
            "0 2550 0.13354817032814026\n",
            "0 2560 0.12626813352108002\n",
            "0 2570 0.060317572206258774\n",
            "0 2580 0.07375136017799377\n",
            "0 2590 0.10328701883554459\n",
            "0 2600 0.13499337434768677\n",
            "0 2610 0.10067474842071533\n",
            "0 2620 0.10111134499311447\n",
            "0 2630 0.10141650587320328\n",
            "0 2640 0.11780931055545807\n",
            "0 2650 0.08133966475725174\n",
            "0 2660 0.11150532215833664\n",
            "0 2670 0.09044265002012253\n",
            "0 2680 0.1156444102525711\n",
            "0 2690 0.08505958318710327\n",
            "0 2700 0.10628985613584518\n",
            "0 2710 0.08464469015598297\n",
            "0 2720 0.10537449270486832\n",
            "0 2730 0.0669487714767456\n",
            "0 2740 0.10896968096494675\n",
            "0 2750 0.11398345232009888\n",
            "0 2760 0.11881474405527115\n",
            "0 2770 0.09246180206537247\n",
            "0 2780 0.10730059444904327\n",
            "0 2790 0.09687267988920212\n",
            "0 2800 0.11603841930627823\n",
            "0 2810 0.10195940732955933\n",
            "0 2820 0.09952164441347122\n",
            "0 2830 0.0777965784072876\n",
            "0 2840 0.10622622817754745\n",
            "0 2850 0.10162732750177383\n",
            "0 2860 0.07551398128271103\n",
            "0 2870 0.1028175875544548\n",
            "0 2880 0.12088540941476822\n",
            "0 2890 0.12460646778345108\n",
            "0 2900 0.09906251728534698\n",
            "0 2910 0.09957847744226456\n",
            "0 2920 0.09978484362363815\n",
            "0 2930 0.10562920570373535\n",
            "0 2940 0.0932537242770195\n",
            "0 2950 0.10708282142877579\n",
            "0 2960 0.10511429607868195\n",
            "0 2970 0.11147309839725494\n",
            "0 2980 0.10055991262197495\n",
            "0 2990 0.1094914898276329\n",
            "0 3000 0.08572321385145187\n",
            "0 3010 0.11848080158233643\n",
            "0 3020 0.13087238371372223\n",
            "0 3030 0.10646224021911621\n",
            "0 3040 0.07938886433839798\n",
            "0 3050 0.10806655883789062\n",
            "0 3060 0.11283284425735474\n",
            "0 3070 0.0743868425488472\n",
            "0 3080 0.09596497565507889\n",
            "0 3090 0.07086169719696045\n",
            "0 3100 0.10832829773426056\n",
            "0 3110 0.10745488852262497\n",
            "0 3120 0.11634894460439682\n",
            "0 3130 0.11625418812036514\n",
            "0 3140 0.09931393712759018\n",
            "0 3150 0.09692798554897308\n",
            "0 3160 0.12896180152893066\n",
            "0 3170 0.0717558041214943\n",
            "0 3180 0.07957553118467331\n",
            "0 3190 0.08223821967840195\n",
            "0 3200 0.1333952397108078\n",
            "0 3210 0.10773684084415436\n",
            "0 3220 0.08919427543878555\n",
            "0 3230 0.08375148475170135\n",
            "0 3240 0.10737532377243042\n",
            "0 3250 0.09756014496088028\n",
            "0 3260 0.13250544667243958\n",
            "0 3270 0.11783730238676071\n",
            "0 3280 0.10211147367954254\n",
            "0 3290 0.09781591594219208\n",
            "0 3300 0.09260028600692749\n",
            "0 3310 0.12286887317895889\n",
            "0 3320 0.08784560859203339\n",
            "0 3330 0.08239249140024185\n",
            "0 3340 0.0900692269206047\n",
            "0 3350 0.0915825292468071\n",
            "0 3360 0.08022411912679672\n",
            "0 3370 0.13286957144737244\n",
            "0 3380 0.08710633218288422\n",
            "0 3390 0.10452377051115036\n",
            "0 3400 0.12992292642593384\n",
            "0 3410 0.1012602373957634\n",
            "0 3420 0.12482783943414688\n",
            "0 3430 0.054301727563142776\n",
            "0 3440 0.11992396414279938\n",
            "0 3450 0.1294926404953003\n",
            "0 3460 0.11612723022699356\n",
            "0 3470 0.08214978128671646\n",
            "0 3480 0.10694684088230133\n",
            "0 3490 0.09451571106910706\n",
            "0 3500 0.06901229172945023\n",
            "0 3510 0.09365575760602951\n",
            "0 3520 0.1029643565416336\n",
            "0 3530 0.11860781162977219\n",
            "0 3540 0.09128155559301376\n",
            "0 3550 0.12526105344295502\n",
            "0 3560 0.07939904183149338\n",
            "0 3570 0.11455965042114258\n",
            "0 3580 0.09707343578338623\n",
            "0 3590 0.08155561983585358\n",
            "0 3600 0.11027004569768906\n",
            "0 3610 0.08274064213037491\n",
            "0 3620 0.0703667476773262\n",
            "0 3630 0.11318837851285934\n",
            "0 3640 0.060825373977422714\n",
            "0 3650 0.1126563549041748\n",
            "0 3660 0.11044616997241974\n",
            "0 3670 0.1019938737154007\n",
            "0 3680 0.08616479486227036\n",
            "0 3690 0.10248928517103195\n",
            "0 3700 0.11054453998804092\n",
            "0 3710 0.09244438260793686\n",
            "0 3720 0.09812356531620026\n",
            "0 3730 0.101627878844738\n",
            "0 3740 0.11765114217996597\n",
            "0 3750 0.11990465968847275\n",
            "0 3760 0.1165115013718605\n",
            "0 3770 0.10495008528232574\n",
            "0 3780 0.06942234188318253\n",
            "0 3790 0.08883805572986603\n",
            "0 3800 0.09848269075155258\n",
            "0 3810 0.13132093846797943\n",
            "0 3820 0.11195999383926392\n",
            "0 3830 0.07571424543857574\n",
            "0 3840 0.08297766000032425\n",
            "0 3850 0.10264617204666138\n",
            "0 3860 0.07984120398759842\n",
            "0 3870 0.08148815482854843\n",
            "0 3880 0.09054021537303925\n",
            "0 3890 0.09216482192277908\n",
            "0 3900 0.11677370220422745\n",
            "0 3910 0.1016756221652031\n",
            "0 3920 0.12582948803901672\n",
            "0 3930 0.11594698578119278\n",
            "0 3940 0.11232030391693115\n",
            "0 3950 0.10900179296731949\n",
            "0 3960 0.11255495995283127\n",
            "0 3970 0.12751302123069763\n",
            "0 3980 0.08335582166910172\n",
            "0 3990 0.07498371601104736\n",
            "0 4000 0.09566070884466171\n",
            "0 4010 0.100810706615448\n",
            "0 4020 0.0983167365193367\n",
            "0 4030 0.11499804258346558\n",
            "0 4040 0.08680976927280426\n",
            "0 4050 0.08940195292234421\n",
            "0 4060 0.10777901858091354\n",
            "0 4070 0.07261127233505249\n",
            "0 4080 0.04761858657002449\n",
            "0 4090 0.1171184554696083\n",
            "0 4100 0.0742412582039833\n",
            "0 4110 0.07126007229089737\n",
            "0 4120 0.043263956904411316\n",
            "0 4130 0.12692564725875854\n",
            "0 4140 0.07679254561662674\n",
            "0 4150 0.0666147992014885\n",
            "0 4160 0.1057778149843216\n",
            "0 4170 0.09800001978874207\n",
            "0 4180 0.08677759021520615\n",
            "0 4190 0.10816448926925659\n",
            "0 4200 0.09824639558792114\n",
            "0 4210 0.07093449681997299\n",
            "0 4220 0.11268577724695206\n",
            "0 4230 0.07108410447835922\n",
            "0 4240 0.11375512927770615\n",
            "0 4250 0.06424865871667862\n",
            "0 4260 0.04665536805987358\n",
            "0 4270 0.08011528849601746\n",
            "0 4280 0.1152687594294548\n",
            "0 4290 0.11271727085113525\n",
            "0 4300 0.09399747103452682\n",
            "0 4310 0.08365441113710403\n",
            "0 4320 0.07001107186079025\n",
            "0 4330 0.0927802324295044\n",
            "0 4340 0.0860600396990776\n",
            "0 4350 0.09112977981567383\n",
            "0 4360 0.1074904352426529\n",
            "0 4370 0.0943187028169632\n",
            "0 4380 0.10124582052230835\n",
            "0 4390 0.10199462622404099\n",
            "0 4400 0.07820434123277664\n",
            "0 4410 0.09410881251096725\n",
            "0 4420 0.09618391841650009\n",
            "0 4430 0.051570016890764236\n",
            "0 4440 0.1205376386642456\n",
            "0 4450 0.11188199371099472\n",
            "0 4460 0.15282268822193146\n",
            "0 4470 0.09215710312128067\n",
            "0 4480 0.09558336436748505\n",
            "0 4490 0.09689326584339142\n",
            "0 4500 0.08436335623264313\n",
            "0 4510 0.07717429846525192\n",
            "0 4520 0.10530411452054977\n",
            "0 4530 0.12153217941522598\n",
            "0 4540 0.08767231553792953\n",
            "0 4550 0.10892968624830246\n",
            "0 4560 0.10522010177373886\n",
            "0 4570 0.11779643595218658\n",
            "0 4580 0.12031383812427521\n",
            "0 4590 0.09308724850416183\n",
            "0 4600 0.08206634223461151\n",
            "0 4610 0.09004761278629303\n",
            "0 4620 0.10584261268377304\n",
            "0 4630 0.11332792043685913\n",
            "0 4640 0.10910947620868683\n",
            "0 4650 0.04737921059131622\n",
            "0 4660 0.12930184602737427\n",
            "0 4670 0.09281814843416214\n",
            "0 4680 0.11526893824338913\n",
            "0 4690 0.1131918653845787\n",
            "0 4700 0.10706841200590134\n",
            "0 4710 0.0927470400929451\n",
            "0 4720 0.08291365206241608\n",
            "0 4730 0.11856867372989655\n",
            "0 4740 0.10137816518545151\n",
            "0 4750 0.126756489276886\n",
            "0 4760 0.08918146789073944\n",
            "0 4770 0.08225848525762558\n",
            "0 4780 0.04506809636950493\n",
            "0 4790 0.08545037358999252\n",
            "0 4800 0.08591420948505402\n",
            "0 4810 0.08183249831199646\n",
            "0 4820 0.10058017075061798\n",
            "0 4830 0.0970810055732727\n",
            "0 4840 0.08958706259727478\n",
            "0 4850 0.10408768802881241\n",
            "0 4860 0.12829895317554474\n",
            "0 4870 0.1115979328751564\n",
            "0 4880 0.1296500712633133\n",
            "0 4890 0.1023874506354332\n",
            "0 4900 0.09605463594198227\n",
            "0 4910 0.10423910617828369\n",
            "0 4920 0.1118038222193718\n",
            "0 4930 0.0934944823384285\n",
            "0 4940 0.09007017314434052\n",
            "0 4950 0.08726993203163147\n",
            "0 4960 0.11574357748031616\n",
            "0 4970 0.12584029138088226\n",
            "0 4980 0.11083716154098511\n",
            "0 4990 0.09185341000556946\n",
            "0 5000 0.07259219884872437\n",
            "0 5010 0.08665093779563904\n",
            "0 5020 0.1029396653175354\n",
            "0 5030 0.11651577055454254\n",
            "0 5040 0.12303633987903595\n",
            "0 5050 0.11559436470270157\n",
            "0 5060 0.11571617424488068\n",
            "0 5070 0.09456837177276611\n",
            "0 5080 0.07191811501979828\n",
            "0 5090 0.08764752745628357\n",
            "0 5100 0.09413166344165802\n",
            "0 5110 0.14113178849220276\n",
            "0 5120 0.08648449927568436\n",
            "0 5130 0.09610910713672638\n",
            "0 5140 0.10610194504261017\n",
            "0 5150 0.07533536851406097\n",
            "0 5160 0.08972670882940292\n",
            "0 5170 0.10483863204717636\n",
            "0 5180 0.11919249594211578\n",
            "0 5190 0.06752462685108185\n",
            "0 5200 0.0780799388885498\n",
            "0 5210 0.08549199253320694\n",
            "0 5220 0.08956675231456757\n",
            "0 5230 0.09441087394952774\n",
            "0 5240 0.09114519506692886\n",
            "0 5250 0.10198114067316055\n",
            "0 5260 0.12158950418233871\n",
            "0 5270 0.12911656498908997\n",
            "0 5280 0.1352892369031906\n",
            "0 5290 0.1004381999373436\n",
            "0 5300 0.08107881247997284\n",
            "0 5310 0.08492894470691681\n",
            "0 5320 0.10075616091489792\n",
            "0 5330 0.10159683227539062\n",
            "0 5340 0.10527022182941437\n",
            "0 5350 0.11768650263547897\n",
            "0 5360 0.10338114947080612\n",
            "0 5370 0.07275529950857162\n",
            "0 5380 0.09532270580530167\n",
            "0 5390 0.12575924396514893\n",
            "0 5400 0.0803881287574768\n",
            "0 5410 0.06341421604156494\n",
            "0 5420 0.12016266584396362\n",
            "0 5430 0.10751273483037949\n",
            "0 5440 0.10584087669849396\n",
            "0 5450 0.10310118645429611\n",
            "0 5460 0.12165230512619019\n",
            "0 5470 0.101225845515728\n",
            "0 5480 0.12202014774084091\n",
            "0 5490 0.09936584532260895\n",
            "0 5500 0.08517064899206161\n",
            "0 5510 0.12664851546287537\n",
            "0 5520 0.0851343497633934\n",
            "0 5530 0.08941003680229187\n",
            "0 5540 0.053653161972761154\n",
            "0 5550 0.13038019835948944\n",
            "0 5560 0.09932612627744675\n",
            "0 5570 0.09526287019252777\n",
            "0 5580 0.12706910073757172\n",
            "0 5590 0.11943419277667999\n",
            "0 5600 0.0718696266412735\n",
            "0 5610 0.09319774806499481\n",
            "0 5620 0.11387137323617935\n",
            "0 5630 0.1347300410270691\n",
            "0 5640 0.06483577191829681\n",
            "0 5650 0.1213536486029625\n",
            "0 5660 0.11426673084497452\n",
            "0 5670 0.1125129833817482\n",
            "0 5680 0.12490810453891754\n",
            "0 5690 0.09285064041614532\n",
            "0 5700 0.0828213095664978\n",
            "0 5710 0.08875411003828049\n",
            "0 5720 0.10571936517953873\n",
            "0 5730 0.09439576417207718\n",
            "0 5740 0.10883256047964096\n",
            "0 5750 0.0773208886384964\n",
            "0 5760 0.1239638701081276\n",
            "0 5770 0.0935199111700058\n",
            "0 5780 0.11149407923221588\n",
            "0 5790 0.09326180070638657\n",
            "0 5800 0.12633772194385529\n",
            "0 5810 0.10311222076416016\n",
            "0 5820 0.08691339939832687\n",
            "0 5830 0.09369941800832748\n",
            "0 5840 0.0612902007997036\n",
            "0 5850 0.09538368135690689\n",
            "0 5860 0.08136551827192307\n",
            "0 5870 0.10449625551700592\n",
            "0 5880 0.12248456478118896\n",
            "0 5890 0.062146224081516266\n",
            "0 5900 0.11438190191984177\n",
            "0 5910 0.08547117561101913\n",
            "0 5920 0.08739922195672989\n",
            "0 5930 0.1191379576921463\n",
            "0 5940 0.08316465467214584\n",
            "0 5950 0.1505756825208664\n",
            "0 5960 0.10028975456953049\n",
            "0 5970 0.1357307881116867\n",
            "0 5980 0.1083139106631279\n",
            "0 5990 0.1100940853357315\n",
            "0 6000 0.11082228273153305\n",
            "0 6010 0.10471005737781525\n",
            "0 6020 0.12467892467975616\n",
            "0 6030 0.08915029466152191\n",
            "0 6040 0.11698102951049805\n",
            "0 6050 0.07959144562482834\n",
            "0 6060 0.09971430897712708\n",
            "0 6070 0.09667915850877762\n",
            "0 6080 0.10985393822193146\n",
            "0 6090 0.12653221189975739\n",
            "0 6100 0.09569282829761505\n",
            "0 6110 0.09424829483032227\n",
            "0 6120 0.09429585188627243\n",
            "0 6130 0.0953434482216835\n",
            "0 6140 0.10992435365915298\n",
            "0 6150 0.05430221185088158\n",
            "0 6160 0.10926999896764755\n",
            "0 6170 0.10340625047683716\n",
            "0 6180 0.12013741582632065\n",
            "0 6190 0.11690857261419296\n",
            "0 6200 0.10604163259267807\n",
            "0 6210 0.09071913361549377\n",
            "0 6220 0.11177791655063629\n",
            "0 6230 0.08903146535158157\n",
            "0 6240 0.09244250506162643\n",
            "0 6250 0.0970892533659935\n",
            "0 6260 0.101381316781044\n",
            "0 6270 0.11502907425165176\n",
            "0 6280 0.0822131559252739\n",
            "0 6290 0.1357150822877884\n",
            "0 6300 0.08812364190816879\n",
            "0 6310 0.08909448981285095\n",
            "0 6320 0.130191370844841\n",
            "0 6330 0.06530503928661346\n",
            "0 6340 0.11772900074720383\n",
            "0 6350 0.0957624539732933\n",
            "0 6360 0.07834018766880035\n",
            "0 6370 0.09578969329595566\n",
            "0 6380 0.129588782787323\n",
            "0 6390 0.09119462221860886\n",
            "0 6400 0.13133810460567474\n",
            "0 6410 0.11792252212762833\n",
            "0 6420 0.1032634750008583\n",
            "0 6430 0.11511214077472687\n",
            "0 6440 0.07551252096891403\n",
            "0 6450 0.12490992993116379\n",
            "0 6460 0.1024557575583458\n",
            "0 6470 0.10265287011861801\n",
            "0 6480 0.08910811692476273\n",
            "0 6490 0.09620080888271332\n",
            "0 6500 0.09263172745704651\n",
            "0 6510 0.1117451936006546\n",
            "0 6520 0.10386983305215836\n",
            "0 6530 0.10419130325317383\n",
            "0 6540 0.10507084429264069\n",
            "0 6550 0.10235823690891266\n",
            "0 6560 0.08519578725099564\n",
            "0 6570 0.10230200737714767\n",
            "0 6580 0.09216323494911194\n",
            "0 6590 0.07833971828222275\n",
            "0 6600 0.08901358395814896\n",
            "0 6610 0.10996001213788986\n",
            "0 6620 0.1112360954284668\n",
            "0 6630 0.0908072218298912\n",
            "0 6640 0.0926300659775734\n",
            "0 6650 0.13710172474384308\n",
            "0 6660 0.0759638324379921\n",
            "0 6670 0.12199093401432037\n",
            "0 6680 0.10152315348386765\n",
            "0 6690 0.12113320827484131\n",
            "0 6700 0.09227893501520157\n",
            "0 6710 0.11057372391223907\n",
            "0 6720 0.09746788442134857\n",
            "0 6730 0.10560878366231918\n",
            "0 6740 0.13452661037445068\n",
            "0 6750 0.0876169428229332\n",
            "0 6760 0.07543158531188965\n",
            "0 6770 0.10224851220846176\n",
            "0 6780 0.14620345830917358\n",
            "0 6790 0.0743599459528923\n",
            "0 6800 0.0798380970954895\n",
            "0 6810 0.12030750513076782\n",
            "0 6820 0.1180659681558609\n",
            "0 6830 0.0917426198720932\n",
            "0 6840 0.12124232202768326\n",
            "0 6850 0.1349683552980423\n",
            "0 6860 0.06880344450473785\n",
            "0 6870 0.11679311841726303\n",
            "0 6880 0.10005233436822891\n",
            "0 6890 0.12716062366962433\n",
            "0 6900 0.10072820633649826\n",
            "0 6910 0.09355929493904114\n",
            "0 6920 0.10539428144693375\n",
            "0 6930 0.12195885181427002\n",
            "0 6940 0.12336810678243637\n",
            "0 6950 0.08427724242210388\n",
            "0 6960 0.1025446206331253\n",
            "0 6970 0.07300984114408493\n",
            "0 6980 0.11032219976186752\n",
            "0 6990 0.1127411276102066\n",
            "0 7000 0.08782421052455902\n",
            "0 7010 0.07048668712377548\n",
            "0 7020 0.1008555218577385\n",
            "0 7030 0.09678687900304794\n",
            "0 7040 0.09466982632875443\n",
            "0 7050 0.07544074207544327\n",
            "0 7060 0.11662282794713974\n",
            "0 7070 0.10086200386285782\n",
            "0 7080 0.11293331533670425\n",
            "0 7090 0.1297297179698944\n",
            "0 7100 0.05955944582819939\n",
            "0 7110 0.13433891534805298\n",
            "0 7120 0.10203631222248077\n",
            "0 7130 0.04120028018951416\n",
            "0 7140 0.12285067141056061\n",
            "0 7150 0.10735634714365005\n",
            "0 7160 0.050958480685949326\n",
            "0 7170 0.07060279697179794\n",
            "0 7180 0.11263119429349899\n",
            "0 7190 0.11008161306381226\n",
            "0 7200 0.0784749910235405\n",
            "0 7210 0.08774765580892563\n",
            "0 7220 0.12964141368865967\n",
            "0 7230 0.10630390793085098\n",
            "0 7240 0.07151710242033005\n",
            "0 7250 0.07650893926620483\n",
            "0 7260 0.08274303376674652\n",
            "0 7270 0.08766577392816544\n",
            "0 7280 0.13715621829032898\n",
            "0 7290 0.10993766784667969\n",
            "0 7300 0.0870981216430664\n",
            "0 7310 0.11777937412261963\n",
            "0 7320 0.12866288423538208\n",
            "0 7330 0.09333712607622147\n",
            "0 7340 0.08672911673784256\n",
            "0 7350 0.10802855342626572\n",
            "0 7360 0.09993129223585129\n",
            "0 7370 0.10902491956949234\n",
            "0 7380 0.08838293701410294\n",
            "0 7390 0.13031718134880066\n",
            "0 7400 0.0700501799583435\n",
            "0 7410 0.1001402959227562\n",
            "0 7420 0.0854405090212822\n",
            "0 7430 0.12405168265104294\n",
            "0 7440 0.11381950229406357\n",
            "0 7450 0.1352577805519104\n",
            "0 7460 0.08698620647192001\n",
            "0 7470 0.08081977814435959\n",
            "0 7480 0.10751596838235855\n",
            "0 7490 0.10141004621982574\n",
            "0 7500 0.08376633375883102\n",
            "0 7510 0.13437716662883759\n",
            "0 7520 0.08383452147245407\n",
            "0 7530 0.07743693888187408\n",
            "0 7540 0.1215353012084961\n",
            "0 7550 0.12094268947839737\n",
            "0 7560 0.08484300225973129\n",
            "0 7570 0.06465470045804977\n",
            "0 7580 0.08396188914775848\n",
            "0 7590 0.11574188619852066\n",
            "0 7600 0.09433959424495697\n",
            "0 7610 0.08430501073598862\n",
            "0 7620 0.10744720697402954\n",
            "0 7630 0.1011907234787941\n",
            "0 7640 0.06822075694799423\n",
            "0 7650 0.1209007054567337\n",
            "0 7660 0.09166721254587173\n",
            "0 7670 0.11094503849744797\n",
            "0 7680 0.08666044473648071\n",
            "0 7690 0.13051266968250275\n",
            "0 7700 0.0848478376865387\n",
            "0 7710 0.10157275199890137\n",
            "0 7720 0.090495765209198\n",
            "0 7730 0.08737170696258545\n",
            "0 7740 0.08200982213020325\n",
            "0 7750 0.06646962463855743\n",
            "0 7760 0.11353536695241928\n",
            "0 7770 0.11608578264713287\n",
            "0 7780 0.10168137401342392\n",
            "0 7790 0.08326411247253418\n",
            "0 7800 0.08149345964193344\n",
            "0 7810 0.10325314104557037\n",
            "0 7820 0.10924940556287766\n",
            "0 7830 0.07045654952526093\n",
            "0 7840 0.11359281837940216\n",
            "0 7850 0.0997486338019371\n",
            "0 7860 0.12627653777599335\n",
            "0 7870 0.123850516974926\n",
            "0 7880 0.08462613821029663\n",
            "0 7890 0.10075192898511887\n",
            "0 7900 0.09879542142152786\n",
            "0 7910 0.12509319186210632\n",
            "0 7920 0.14411257207393646\n",
            "0 7930 0.13040511310100555\n",
            "0 7940 0.072821706533432\n",
            "0 7950 0.11724704504013062\n",
            "0 7960 0.09211990237236023\n",
            "0 7970 0.11354023218154907\n",
            "0 7980 0.10001104325056076\n",
            "0 7990 0.09504587948322296\n",
            "0 8000 0.11936075985431671\n",
            "0 8010 0.11660324782133102\n",
            "0 8020 0.08235830068588257\n",
            "0 8030 0.07384143024682999\n",
            "0 8040 0.07914973795413971\n",
            "0 8050 0.0893176719546318\n",
            "0 8060 0.09123372286558151\n",
            "0 8070 0.054267216473817825\n",
            "0 8080 0.08030936866998672\n",
            "0 8090 0.09809765964746475\n",
            "0 8100 0.13246354460716248\n",
            "0 8110 0.07354298233985901\n",
            "0 8120 0.1417120397090912\n",
            "0 8130 0.1234181672334671\n",
            "0 8140 0.10186964273452759\n",
            "0 8150 0.12045379728078842\n",
            "0 8160 0.05322180315852165\n",
            "0 8170 0.11135663092136383\n",
            "0 8180 0.10221274942159653\n",
            "0 8190 0.0897754654288292\n",
            "0 8200 0.11396662145853043\n",
            "0 8210 0.10131299495697021\n",
            "0 8220 0.10161500424146652\n",
            "0 8230 0.0963691994547844\n",
            "0 8240 0.10411971062421799\n",
            "0 8250 0.11152738332748413\n",
            "0 8260 0.09460723400115967\n",
            "0 8270 0.10623019188642502\n",
            "0 8280 0.07772815227508545\n",
            "0 8290 0.09929531067609787\n",
            "0 8300 0.10424889624118805\n",
            "0 8310 0.11913825571537018\n",
            "0 8320 0.12090469896793365\n",
            "0 8330 0.08681754022836685\n",
            "0 8340 0.0863415002822876\n",
            "0 8350 0.1284763216972351\n",
            "0 8360 0.09352515637874603\n",
            "0 8370 0.11777955293655396\n",
            "0 8380 0.08614164590835571\n",
            "0 8390 0.1281924694776535\n",
            "0 8400 0.06406145542860031\n",
            "0 8410 0.07730179280042648\n",
            "0 8420 0.08469000458717346\n",
            "0 8430 0.09636678546667099\n",
            "0 8440 0.1176985651254654\n",
            "0 8450 0.09241299331188202\n",
            "0 8460 0.11485760658979416\n",
            "0 8470 0.08178852498531342\n",
            "0 8480 0.08795972168445587\n",
            "0 8490 0.10214598476886749\n",
            "0 8500 0.11165942251682281\n",
            "0 8510 0.0583634190261364\n",
            "0 8520 0.10074964910745621\n",
            "0 8530 0.10673075169324875\n",
            "0 8540 0.13062560558319092\n",
            "0 8550 0.08878934383392334\n",
            "0 8560 0.11034411191940308\n",
            "0 8570 0.06753017008304596\n",
            "0 8580 0.11680924147367477\n",
            "0 8590 0.1314311921596527\n",
            "0 8600 0.11476091295480728\n",
            "0 8610 0.1266382783651352\n",
            "0 8620 0.1080024242401123\n",
            "0 8630 0.04864410683512688\n",
            "0 8640 0.12787126004695892\n",
            "0 8650 0.1053580790758133\n",
            "0 8660 0.08881010860204697\n",
            "0 8670 0.07411214709281921\n",
            "0 8680 0.0983961746096611\n",
            "0 8690 0.07703646272420883\n",
            "0 8700 0.07460589706897736\n",
            "0 8710 0.11500083655118942\n",
            "0 8720 0.11049437522888184\n",
            "0 8730 0.08248984068632126\n",
            "0 8740 0.07582826912403107\n",
            "0 8750 0.10655864328145981\n",
            "0 8760 0.10471444576978683\n",
            "0 8770 0.11484798043966293\n",
            "0 8780 0.09400838613510132\n",
            "0 8790 0.10052899271249771\n",
            "0 8800 0.0881684422492981\n",
            "0 8810 0.10212178528308868\n",
            "0 8820 0.12470579147338867\n",
            "0 8830 0.10085958242416382\n",
            "0 8840 0.11866450309753418\n",
            "0 8850 0.10781577974557877\n",
            "0 8860 0.09452937543392181\n",
            "0 8870 0.11465555429458618\n",
            "0 8880 0.10293231159448624\n",
            "0 8890 0.09592543542385101\n",
            "0 8900 0.09668739885091782\n",
            "0 8910 0.08328001946210861\n",
            "0 8920 0.09981968253850937\n",
            "0 8930 0.10996868461370468\n",
            "0 8940 0.09681212157011032\n",
            "0 8950 0.085533007979393\n",
            "0 8960 0.10075249522924423\n",
            "0 8970 0.13523630797863007\n",
            "0 8980 0.09957768023014069\n",
            "0 8990 0.07208795845508575\n",
            "0 9000 0.07702743262052536\n",
            "0 9010 0.12318936735391617\n",
            "0 9020 0.07884170860052109\n",
            "0 9030 0.07516200840473175\n",
            "0 9040 0.08930912613868713\n",
            "0 9050 0.11444064229726791\n",
            "0 9060 0.088164784014225\n",
            "0 9070 0.07841038703918457\n",
            "0 9080 0.12738557159900665\n",
            "0 9090 0.08440714329481125\n",
            "0 9100 0.07176319509744644\n",
            "0 9110 0.12279579788446426\n",
            "0 9120 0.10318698734045029\n",
            "0 9130 0.09985673427581787\n",
            "0 9140 0.0952882170677185\n",
            "0 9150 0.08160587400197983\n",
            "0 9160 0.09678936749696732\n",
            "0 9170 0.11355860531330109\n",
            "0 9180 0.10524643957614899\n",
            "0 9190 0.12943927943706512\n",
            "0 9200 0.08276429027318954\n",
            "0 9210 0.09236191213130951\n",
            "0 9220 0.1272054761648178\n",
            "0 9230 0.09713758528232574\n",
            "0 9240 0.12965010106563568\n",
            "0 9250 0.11126051098108292\n",
            "0 9260 0.05831092596054077\n",
            "0 9270 0.12701930105686188\n",
            "0 9280 0.08026035875082016\n",
            "0 9290 0.10128685086965561\n",
            "0 9300 0.08996887505054474\n",
            "0 9310 0.11244417726993561\n",
            "0 9320 0.08861719816923141\n",
            "0 9330 0.1194080039858818\n",
            "0 9340 0.11551027745008469\n",
            "0 9350 0.11268136650323868\n",
            "0 9360 0.0808228999376297\n",
            "0 9370 0.12002795934677124\n",
            "0 9380 0.10737931728363037\n",
            "0 9390 0.12708845734596252\n",
            "0 9400 0.09880887717008591\n",
            "0 9410 0.0656343400478363\n",
            "0 9420 0.07533460855484009\n",
            "0 9430 0.08744287490844727\n",
            "0 9440 0.11961700022220612\n",
            "0 9450 0.08247946947813034\n",
            "0 9460 0.11393685638904572\n",
            "0 9470 0.09923258423805237\n",
            "0 9480 0.1203518733382225\n",
            "0 9490 0.05769026279449463\n",
            "0 9500 0.10188744217157364\n",
            "0 9510 0.0979938879609108\n",
            "0 9520 0.11341970413923264\n",
            "0 9530 0.091485396027565\n",
            "0 9540 0.09380438923835754\n",
            "0 9550 0.09159937500953674\n",
            "0 9560 0.09192084521055222\n",
            "0 9570 0.10743916034698486\n",
            "0 9580 0.07380800694227219\n",
            "0 9590 0.1121426597237587\n",
            "0 9600 0.09763577580451965\n",
            "0 9610 0.10572396963834763\n",
            "0 9620 0.08071371912956238\n",
            "0 9630 0.08401089161634445\n",
            "0 9640 0.0838504359126091\n",
            "0 9650 0.1100701317191124\n",
            "0 9660 0.09544205665588379\n",
            "0 9670 0.13049356639385223\n",
            "0 9680 0.10699508339166641\n",
            "0 9690 0.0781802162528038\n",
            "0 9700 0.10938891023397446\n",
            "0 9710 0.1069280281662941\n",
            "0 9720 0.07127862423658371\n",
            "0 9730 0.06558474153280258\n",
            "0 9740 0.10642924159765244\n",
            "0 9750 0.09570186585187912\n",
            "0 9760 0.12366040796041489\n",
            "0 9770 0.11241128295660019\n",
            "0 9780 0.08393097668886185\n",
            "0 9790 0.10141310840845108\n",
            "0 9800 0.08037296682596207\n",
            "0 9810 0.06767996400594711\n",
            "0 9820 0.09493937343358994\n",
            "0 9830 0.1283894032239914\n",
            "0 9840 0.0863681510090828\n",
            "0 9850 0.10071679204702377\n",
            "0 9860 0.06659270077943802\n",
            "0 9870 0.09425308555364609\n",
            "0 9880 0.09606485813856125\n",
            "0 9890 0.10036414861679077\n",
            "0 9900 0.10476239025592804\n",
            "0 9910 0.08001244813203812\n",
            "0 9920 0.09701582044363022\n",
            "0 9930 0.10125767439603806\n",
            "0 9940 0.11114083975553513\n",
            "0 9950 0.10907631367444992\n",
            "0 9960 0.12587741017341614\n",
            "0 9970 0.07187696546316147\n",
            "0 9980 0.07609888166189194\n",
            "0 9990 0.10963840782642365\n",
            "0 10000 0.10955486446619034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取索引\n",
        "indices = list(range(len(train_loss)))\n",
        "\n",
        "# 绘制数据\n",
        "# for i in range(len(loss_lists)):\n",
        "plt.plot(indices, train_loss, marker='o')\n",
        "\n",
        "# 添加标题和标签\n",
        "plt.title('Plot of Float List')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "\n",
        "# 显示图形\n",
        "plt.show()\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "8aDItbccONN6",
        "outputId": "e95bbf0b-7ac2-44c0-87f6-4d5b2bdf0b7f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfI3n0XuyqG6",
        "outputId": "30b5821b-a615-4c4c-88c5-8c16bf827da4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jan 10 06:02:19 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P0              31W /  70W |    167MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f-78-DsezBC4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MoCoCIFAR10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}